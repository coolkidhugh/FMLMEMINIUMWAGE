import streamlit as st
import pandas as pd
import numpy as np
import re
from utils import to_excel
from config import CTRIP_DATE_SYSTEM_COLUMNS, CTRIP_DATE_CTRIP_COLUMNS, CTRIP_AUDIT_CTRIP_COLUMN_MAP, CTRIP_AUDIT_SYSTEM_COLUMN_MAP

# ==============================================================================
# --- [APP] Ctrip Date Comparison Tool ---
# ==============================================================================

def run_ctrip_date_comparison_app():
    """Renders the UI for the Ctrip Date Comparison tool."""
    st.title("é‡‘é™µå·¥å…·ç®± - æºç¨‹å¯¹æ—¥æœŸ")
    st.markdown("""
    æ­¤å·¥å…·ç”¨äºæ¯”å¯¹ **ç³»ç»Ÿè®¢å• (System Order)** å’Œ **æºç¨‹è®¢å• (Ctrip Order)**ã€‚
    1.  è¯·åˆ†åˆ«ä¸Šä¼ ä¸¤ä¸ªå¯¹åº”çš„ Excel æ–‡ä»¶ã€‚
    2.  å·¥å…·ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶ç»Ÿä¸€ä¸¤ç§ä¸åŒçš„æ—¥æœŸæ ¼å¼ (`YYMMDD` å’Œ `YYYY/MM/DD`)ã€‚
    3.  ç‚¹å‡»â€œå¼€å§‹æ¯”å¯¹â€ï¼Œä¸‹æ–¹å°†æ˜¾ç¤ºç»“æœæ‘˜è¦ï¼Œå¹¶æä¾›è¯¦ç»†æŠ¥å‘Šä¸‹è½½ã€‚
    """)

    # --- File Upload ---
    col1, col2 = st.columns(2)
    with col1:
        system_file_uploaded = st.file_uploader("ä¸Šä¼ æ‚¨çš„ System Order (.xlsx)", type=["xlsx"], key="system_uploader")
    with col2:
        ctrip_file_uploaded = st.file_uploader("ä¸Šä¼ æ‚¨çš„ Ctrip Order (.xlsx)", type=["xlsx"], key="ctrip_uploader")

    # --- Comparison Logic ---
    if st.button("å¼€å§‹æ¯”å¯¹", type="primary", disabled=(not system_file_uploaded or not ctrip_file_uploaded)):
        results = perform_date_comparison(system_file_uploaded, ctrip_file_uploaded)

        if isinstance(results, str): # Handle error messages
            st.error(results)
        elif results:
            date_mismatch_df, not_found_df = results
            st.success("æ¯”å¯¹å®Œæˆï¼")
            
            st.header("ç»“æœæ‘˜è¦")
            col1, col2 = st.columns(2)
            col1.metric("âš ï¸ æ—¥æœŸä¸åŒ¹é…çš„è®¢å•", f"{len(date_mismatch_df)} æ¡")
            col2.metric("â„¹ï¸ åœ¨æºç¨‹ä¸­æœªæ‰¾åˆ°çš„è®¢å•", f"{len(not_found_df)} æ¡")

            df_to_download = {
                "æ—¥æœŸä¸åŒ¹é…çš„è®¢å•": date_mismatch_df,
                "åœ¨Ctripä¸­æœªæ‰¾åˆ°çš„è®¢å•": not_found_df
            }
            excel_data = to_excel(df_to_download)
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½è¯¦ç»†æ¯”å¯¹æŠ¥å‘Š (.xlsx)",
                data=excel_data,
                file_name="æºç¨‹æ—¥æœŸæ¯”å¯¹æŠ¥å‘Š.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

            st.header("ç»“æœè¯¦æƒ…")
            with st.expander(f"æŸ¥çœ‹ {len(date_mismatch_df)} æ¡æ—¥æœŸä¸åŒ¹é…çš„è®¢å•", expanded=not date_mismatch_df.empty):
                if not date_mismatch_df.empty:
                    st.dataframe(date_mismatch_df)
                else:
                    st.info("æ²¡æœ‰å‘ç°æ—¥æœŸä¸åŒ¹é…çš„è®¢å•ã€‚")
            
            with st.expander(f"æŸ¥çœ‹ {len(not_found_df)} æ¡åœ¨æºç¨‹ä¸­æœªæ‰¾åˆ°çš„è®¢å•"):
                if not not_found_df.empty:
                    st.dataframe(not_found_df)
                else:
                    st.info("æ‰€æœ‰ç³»ç»Ÿè®¢å•éƒ½èƒ½åœ¨æºç¨‹è®¢å•ä¸­æ‰¾åˆ°ã€‚")

# --- Helper function for Date Comparison ---
@st.cache_data
def perform_date_comparison(_system_file_buffer, _ctrip_file_buffer):
    """Core logic to compare two dataframes for date mismatches."""
    
    def clean_data(file_buffer, cols_map, date_format=None):
        try:
            df = pd.read_excel(file_buffer)
            required_cols = list(cols_map.values())
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                return f"æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {', '.join(missing_cols)}"

            df_selected = df[required_cols].copy()
            df_selected.columns = ['é¢„å®šå·', 'å…¥ä½æ—¥æœŸ', 'ç¦»åº—æ—¥æœŸ']
            
            df_selected['é¢„å®šå·'] = df_selected['é¢„å®šå·'].astype(str).str.strip().str.upper()
            
            date_cols = ['å…¥ä½æ—¥æœŸ', 'ç¦»åº—æ—¥æœŸ']
            for col in date_cols:
                if date_format:
                    df_selected[col] = pd.to_datetime(df_selected[col], format=date_format, errors='coerce').dt.date
                else:
                    df_selected[col] = pd.to_datetime(df_selected[col], errors='coerce').dt.date
            
            df_selected.dropna(subset=['é¢„å®šå·'] + date_cols, inplace=True)
            return df_selected
        except Exception as e:
            return f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}"

    df_system = clean_data(_system_file_buffer, CTRIP_DATE_SYSTEM_COLUMNS, date_format='%y%m%d')
    if isinstance(df_system, str): return df_system

    df_ctrip = clean_data(_ctrip_file_buffer, CTRIP_DATE_CTRIP_COLUMNS)
    if isinstance(df_ctrip, str): return df_ctrip
    
    merged_df = pd.merge(df_system, df_ctrip, on='é¢„å®šå·', how='left', suffixes=('_ç³»ç»Ÿ', '_Ctrip'))

    not_found_df = merged_df[merged_df['å…¥ä½æ—¥æœŸ_Ctrip'].isnull()][['é¢„å®šå·', 'å…¥ä½æ—¥æœŸ_ç³»ç»Ÿ', 'ç¦»åº—æ—¥æœŸ_ç³»ç»Ÿ']]
    
    found_df = merged_df.dropna(subset=['å…¥ä½æ—¥æœŸ_Ctrip']).copy()
    
    date_mismatch_df = found_df[
        (found_df['å…¥ä½æ—¥æœŸ_ç³»ç»Ÿ'] != found_df['å…¥ä½æ—¥æœŸ_Ctrip']) |
        (found_df['ç¦»åº—æ—¥æœŸ_ç³»ç»Ÿ'] != found_df['ç¦»åº—æ—¥æœŸ_Ctrip'])
    ][['é¢„å®šå·', 'å…¥ä½æ—¥æœŸ_ç³»ç»Ÿ', 'ç¦»åº—æ—¥æœŸ_ç³»ç»Ÿ', 'å…¥ä½æ—¥æœŸ_Ctrip', 'ç¦»åº—æ—¥æœŸ_Ctrip']]
    
    return date_mismatch_df, not_found_df

# ==============================================================================
# --- [APP] Ctrip Audit Tool ---
# ==============================================================================

def run_ctrip_audit_app():
    """Renders the UI for the Ctrip Audit tool."""
    st.title("é‡‘é™µå·¥å…·ç®± - æºç¨‹å®¡å•")
    st.markdown("""
    æ­¤å·¥å…·ç”¨äºæ ¹æ® **ç³»ç»Ÿå¯¼å‡ºçš„è®¢å•** æ¥å®¡æ ¸ **æºç¨‹è®¢å•** çš„ç¦»åº—æ—¶é—´å’Œæˆ¿å·ã€‚
    1.  è¯·åˆ†åˆ«ä¸Šä¼  **æºç¨‹è®¢å•Excel** å’Œ **ç³»ç»Ÿè®¢å•Excel**ã€‚
    2.  å·¥å…·å°†æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§è¿›è¡Œä¸‰è½®åŒ¹é…ï¼š
        - (1) **ç¬¬ä¸‰æ–¹é¢„è®¢å·**
        - (2) **ç¡®è®¤å·/é¢„è®¢å·**
        - (3) **å®¢äººå§“å**
    3.  æœ€ç»ˆç”ŸæˆåŒ…å« `è®¢å•å·`, `å®¢äººå§“å`, `åˆ°è¾¾`, `ç¦»å¼€`, `æˆ¿å·`, `çŠ¶æ€` çš„å®¡æ ¸ç»“æœã€‚
    """)

    # --- File Upload ---
    col1, col2 = st.columns(2)
    with col1:
        ctrip_file_uploaded = st.file_uploader("ä¸Šä¼ æºç¨‹è®¢å•.xlsx", type=["xlsx"], key="ctrip_audit_uploader_final")
    with col2:
        system_file_uploaded = st.file_uploader("ä¸Šä¼ ç³»ç»Ÿè®¢å•.xlsx", type=["xlsx"], key="system_audit_uploader_final")

    if st.button("å¼€å§‹å®¡æ ¸", type="primary", disabled=(not ctrip_file_uploaded or not system_file_uploaded)):
        with st.spinner("æ­£åœ¨æ‰§è¡Œä¸‰è½®åŒ¹é…ä¸å®¡æ ¸..."):
            result = perform_audit(ctrip_file_uploaded, system_file_uploaded)

            if isinstance(result, str):
                st.error(result)
            else:
                st.success("å®¡æ ¸å®Œæˆï¼")
                st.dataframe(result)
                
                excel_data_audit = to_excel({"å®¡æ ¸ç»“æœ": result})
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½å®¡æ ¸ç»“æœ (.xlsx)",
                    data=excel_data_audit,
                    file_name="matched_orders.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key="download-audit-final"
                )

# --- Helper function for Audit ---
def perform_audit(ctrip_buffer, system_buffer):
    """Core logic for the three-round audit process."""
    def clean_confirmation_number(number):
        if pd.isna(number): return None
        digits = re.findall(r'\d+', str(number))
        return ''.join(digits) if digits else None

    def clean_third_party_number(number):
        if pd.isna(number): return None
        return re.sub(r'R\d+$', '', str(number).strip())
    
    def find_and_rename_columns(df, column_map):
        """Finds columns from a list of possibles and renames to a standard name."""
        missing_cols = []
        for standard_name, possible_names in column_map.items():
            found_col = next((name for name in possible_names if name in df.columns), None)
            if not found_col: # Try fuzzy match if exact match fails
                found_col = next((col for col in df.columns for name in possible_names if name in col), None)

            if found_col:
                df.rename(columns={found_col: standard_name}, inplace=True)
            else:
                missing_cols.append(standard_name)
        return missing_cols

    try:
        ctrip_df = pd.read_excel(ctrip_buffer, dtype={'è®¢å•å·': str, 'ç¡®è®¤å·': str})
        system_df = pd.read_excel(system_buffer, dtype={'é¢„è®¢å·': str, 'ç¬¬ä¸‰æ–¹é¢„å®šå·': str})

        if ctrip_df.empty: return "é”™è¯¯: ä¸Šä¼ çš„æºç¨‹è®¢å•æ–‡ä»¶ä¸ºç©ºã€‚"
        if system_df.empty: return "é”™è¯¯: ä¸Šä¼ çš„ç³»ç»Ÿè®¢å•æ–‡ä»¶ä¸ºç©ºã€‚"

        ctrip_df.columns = ctrip_df.columns.str.strip()
        system_df.columns = system_df.columns.str.strip()

        missing_ctrip = find_and_rename_columns(ctrip_df, CTRIP_AUDIT_CTRIP_COLUMN_MAP)
        if missing_ctrip: return f"é”™è¯¯: æºç¨‹æ–‡ä»¶ç¼ºå°‘åˆ—: {', '.join(missing_ctrip)}"
        
        missing_system = find_and_rename_columns(system_df, CTRIP_AUDIT_SYSTEM_COLUMN_MAP)
        if missing_system: return f"é”™è¯¯: ç³»ç»Ÿæ–‡ä»¶ç¼ºå°‘åˆ—: {', '.join(missing_system)}"
        
        # --- Data Preparation ---
        ctrip_df.rename(columns={'å®¢äººå§“å': 'å§“å'}, inplace=True)
        ctrip_df['å§“å'] = ctrip_df['å§“å'].astype(str).str.strip()
        system_df['å§“å'] = system_df['å§“å'].astype(str).str.strip()

        ctrip_df['çº¯æ•°å­—ç¡®è®¤å·'] = ctrip_df['ç¡®è®¤å·'].apply(clean_confirmation_number)
        system_df['æ¸…æ´—åç¬¬ä¸‰æ–¹é¢„å®šå·'] = system_df['ç¬¬ä¸‰æ–¹é¢„å®šå·'].apply(clean_third_party_number)

        system_df.drop_duplicates(subset=['é¢„è®¢å·', 'å§“å', 'æ¸…æ´—åç¬¬ä¸‰æ–¹é¢„å®šå·'], keep='first', inplace=True)
        
        # --- Matching Logic ---
        # Round 1: Third-party booking number
        merged1 = pd.merge(
            ctrip_df, 
            system_df, 
            left_on='è®¢å•å·', 
            right_on='æ¸…æ´—åç¬¬ä¸‰æ–¹é¢„å®šå·', 
            how='left', 
            suffixes=('', '_sys1')
        )
        
        # Round 2: Confirmation number
        unmatched1 = merged1[merged1['é¢„è®¢å·'].isna()].drop(columns=[c for c in merged1.columns if '_sys1' in c])
        merged2 = pd.merge(
            unmatched1,
            system_df,
            left_on='çº¯æ•°å­—ç¡®è®¤å·',
            right_on='é¢„è®¢å·',
            how='left',
            suffixes=('', '_sys2')
        )

        # Round 3: Guest name
        unmatched2 = merged2[merged2['é¢„è®¢å·_sys2'].isna()].drop(columns=[c for c in merged2.columns if '_sys2' in c])
        merged3 = pd.merge(
            unmatched2,
            system_df,
            on='å§“å',
            how='left',
            suffixes=('', '_sys3')
        )

        # --- Combine Results ---
        matched1 = merged1.dropna(subset=['é¢„è®¢å·'])
        matched2 = merged2.dropna(subset=['é¢„è®¢å·_sys2'])
        matched3 = merged3.dropna(subset=['é¢„è®¢å·_sys3'])
        
        # Standardize columns before combining
        for df, suffix in [(matched1, ''), (matched2, '_sys2'), (matched3, '_sys3')]:
            df.rename(columns={
                f'ç¦»å¼€{suffix}': 'åŒ¹é…çš„ç¦»å¼€æ—¶é—´',
                f'æˆ¿å·{suffix}': 'åŒ¹é…çš„æˆ¿å·',
                f'çŠ¶æ€{suffix}': 'åŒ¹é…çš„çŠ¶æ€'
            }, inplace=True)

        final_df = pd.concat([
            matched1, matched2, matched3, 
            merged3[merged3['é¢„è®¢å·_sys3'].isna()] # Unmatched from round 3
        ], ignore_index=True, sort=False)
        
        # Update original values with matched values
        final_df['ç¦»å¼€'] = final_df['åŒ¹é…çš„ç¦»å¼€æ—¶é—´'].fillna(final_df['ç¦»å¼€'])
        final_df['æˆ¿å·'] = final_df['åŒ¹é…çš„æˆ¿å·'].fillna(np.nan)
        final_df['çŠ¶æ€'] = final_df['åŒ¹é…çš„çŠ¶æ€'].fillna('æœªåŒ¹é…')

        return final_df[['è®¢å•å·', 'å§“å', 'åˆ°è¾¾', 'ç¦»å¼€', 'æˆ¿å·', 'çŠ¶æ€']]
        
    except Exception as e:
        return f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"

